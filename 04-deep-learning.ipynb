{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brexit Polarity Tweets - Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8kzg4h68OI0"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "q_loXZ40-3bU"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "# Data Manipulation and Visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLP Tools\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# ML Tools\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, GRU\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CfCKKAjq_G-P",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# path to data\n",
    "PATH_DATA         = \"./data/\"\n",
    "PATH_TWEETS_TOKEN = PATH_DATA + \"preprocessed/tweets_token.csv\"\n",
    "PATH_TARGETS      = PATH_DATA + \"preprocessed/targets.csv\"\n",
    "PATH_TWEETS_RAW   = PATH_DATA + \"preprocessed/tweets_raw.csv\"\n",
    "PATH_GLOVE        = PATH_DATA + 'embeddings/glove.6B.100d.txt'\n",
    "\n",
    "# paths to model checkpoints\n",
    "PATH_MODEL      = \"./model/\"\n",
    "PATH_HISTORY    = PATH_MODEL + \"history/history.pkl\"\n",
    "PATH_CHECKPOINT = PATH_MODEL + \"checkpoint/cp-{epoch:02d}.ckpt\"\n",
    "\n",
    "DIRNAME_CHECKPOINT = os.path.dirname(PATH_CHECKPOINT)\n",
    "\n",
    "# settings\n",
    "N_WORDS        = 30_000\n",
    "N_ROWS         = 100_000\n",
    "EMBEDDING_DIMS = 100\n",
    "TEST_SPLIT     = 0.10\n",
    "VAL_SPLIT      = 0.10 / 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nScPCQAM8nR-"
   },
   "source": [
    "# 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "tweet_tokens = pd.read_csv(PATH_TWEETS_TOKEN, index_col= False)[\"Hit Sentence\"]\n",
    "tweet_tokens = tweet_tokens.map(literal_eval)\n",
    "\n",
    "targets = pd.read_csv(PATH_TARGETS)[\"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 89559 tweets in the train dataset.\n",
      "There are 9951 tweets in the test dataset.\n"
     ]
    }
   ],
   "source": [
    "# generate indices to split dataset\n",
    "sss = ShuffleSplit(n_splits = 1, test_size = TEST_SPLIT, random_state = 123)\n",
    "sss.get_n_splits(tweet_tokens, targets)\n",
    "\n",
    "train_index, test_index = next(sss.split(tweet_tokens, targets))\n",
    "\n",
    "print(f\"There are {len(train_index)} tweets in the train dataset.\")\n",
    "print(f\"There are {len(test_index)} tweets in the test dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 79608 tweets in the train dataset.\n",
      "There are 9951 tweets in the test dataset.\n"
     ]
    }
   ],
   "source": [
    "# generate indices to split dataset\n",
    "sss = ShuffleSplit(n_splits = 1, test_size = VAL_SPLIT, random_state = 123)\n",
    "sss.get_n_splits(tweet_tokens[train_index], targets[train_index])\n",
    "\n",
    "part_train_index, val_index = next(sss.split(tweet_tokens[train_index], targets[train_index]))\n",
    "\n",
    "print(f\"There are {len(part_train_index)} tweets in the train dataset.\")\n",
    "print(f\"There are {len(val_index)} tweets in the test dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(text):\n",
    "    output = []\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    tokenizer  = nltk.WhitespaceTokenizer()\n",
    "    unwanted_tokens = nltk.corpus.stopwords.words(\"english\")\n",
    "    \n",
    "    patterns = [\n",
    "        r\"(?:RT|QT):? ?@[\\w]+:?\",\n",
    "        r\"https?://\\S+\",\n",
    "        r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\",\n",
    "        r\"[$|Â£]?.?[0-9]+(?:,?[0-9]{3})*(?:\\.[0-9]*)*%?(?:st|nd|rd|th)?\",\n",
    "        r\"[0-9]+/[0-9]+\",\n",
    "        u\"\\U0001F600-\\U0001F64F\",  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\",  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\",  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\",  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\",  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\",\n",
    "        u\"\\U000024C2-\\U0001F251\",\n",
    "        u\"\\U0001f926-\\U0001f937\",\n",
    "        u\"\\U00010000-\\U0010ffff\",\n",
    "        u\"\\u2640-\\u2642\",\n",
    "        u\"\\u2600-\\u2B55\",\n",
    "        u\"\\u200d\",\n",
    "        u\"\\u23cf\",\n",
    "        u\"\\u23e9\",\n",
    "        u\"\\u231a\",\n",
    "        u\"\\ufe0f\",  # dingbats\n",
    "        u\"\\u3030\",\n",
    "        \"[\" + string.punctuation + \"]\"\n",
    "    ]\n",
    "    \n",
    "    pattern = \"(\" + \"|\".join(patterns)+ \")\"\n",
    "    \n",
    "    text = re.sub(pattern, \"\", text)\n",
    "    text = re.sub(u\"[\\u2018|\\u2019]\", \"'\", text)\n",
    "    text = re.sub(u\"[\\u201c|\\u201d]\", \"\\\"\", text)\n",
    "    \n",
    "    for token in tokenizer.tokenize(text.lower()):\n",
    "        if token in unwanted_tokens: continue\n",
    "        token = lemmatizer.lemmatize(token)\n",
    "        \n",
    "        if len(token) != 0:\n",
    "            output.append(token)\n",
    "\n",
    "    return \" \".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        value brexit tory mp back constituency weekend...\n",
       "1        michael fabricant absolutely right dangerous e...\n",
       "2        stokiedre find rich tory voter assume unionist...\n",
       "3        brexit britain win london named best city youn...\n",
       "4        afneil johnson taken tory likelihood imposed p...\n",
       "                               ...                        \n",
       "99505    mikegalsworthy brexiteers never interested fac...\n",
       "99506    brexit disaster often obscured covid dover lor...\n",
       "99507    trying shift blame yet position caused greedy ...\n",
       "99508      remember rees mogg said brexit mean lower price\n",
       "99509    reporting observer guardian journalist carole ...\n",
       "Name: Hit Sentence, Length: 99510, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_raw = pd.read_csv(PATH_TWEETS_RAW, index_col= False)[\"Hit Sentence\"]\n",
    "tweet_tokens = tweet_raw.map(process_tweet)\n",
    "tweet_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brexit', 57016),\n",
       " ('eu', 35752),\n",
       " ('uk', 19254),\n",
       " ('boris', 13110),\n",
       " ('people', 10829)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = nltk.FreqDist()\n",
    "for token_list in tweet_tokens:\n",
    "    for token in token_list.split(\" \"):\n",
    "        fd[token] += 1\n",
    "        \n",
    "fd.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "len_oov    = 1\n",
    "len_seq    = 100\n",
    "\n",
    "vocab   = [token for token, count in fd.most_common(vocab_size)]\n",
    "values  = range(2, len(vocab) + 2)\n",
    "\n",
    "init = tf.lookup.KeyValueTensorInitializer(vocab,\n",
    "                                           values,\n",
    "                                           key_dtype = tf.string,\n",
    "                                           value_dtype = tf.int64)\n",
    "\n",
    "vocab_table = tf.lookup.StaticVocabularyTable(init, len_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens  = vocab_size + 2,\n",
    "    output_mode = 'int',\n",
    "    vocabulary  = vocab,\n",
    "    output_sequence_length = len_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "vocab_size = len(vocab) + 2\n",
    "model = tf.keras.models.Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_shape = (100,), name=\"embedding\"),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation = \"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocess_layer(tweet_tokens[train_index])\n",
    "X_val   = preprocess_layer(tweet_tokens[val_index])\n",
    "X_test  = preprocess_layer(tweet_tokens[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = label_encoder.fit_transform(targets)\n",
    "y_train = targets[train_index]\n",
    "y_test  = targets[test_index]\n",
    "y_val   = targets[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=\"BinaryCrossentropy\",\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2799/2799 [==============================] - 15s 5ms/step - loss: 0.3900 - accuracy: 0.8275 - val_loss: 0.2706 - val_accuracy: 0.8936\n",
      "Epoch 2/10\n",
      "2799/2799 [==============================] - 13s 4ms/step - loss: 0.2808 - accuracy: 0.8878 - val_loss: 0.2609 - val_accuracy: 0.8956\n",
      "Epoch 3/10\n",
      "2799/2799 [==============================] - 13s 5ms/step - loss: 0.2593 - accuracy: 0.8982 - val_loss: 0.2355 - val_accuracy: 0.9102\n",
      "Epoch 4/10\n",
      "2799/2799 [==============================] - 13s 5ms/step - loss: 0.2466 - accuracy: 0.9044 - val_loss: 0.2279 - val_accuracy: 0.9137\n",
      "Epoch 5/10\n",
      "2799/2799 [==============================] - 13s 5ms/step - loss: 0.2365 - accuracy: 0.9082 - val_loss: 0.2207 - val_accuracy: 0.9146\n",
      "Epoch 6/10\n",
      "2799/2799 [==============================] - 13s 4ms/step - loss: 0.2279 - accuracy: 0.9106 - val_loss: 0.2390 - val_accuracy: 0.9034\n",
      "Epoch 7/10\n",
      "2799/2799 [==============================] - 12s 4ms/step - loss: 0.2196 - accuracy: 0.9134 - val_loss: 0.2161 - val_accuracy: 0.9131\n",
      "Epoch 8/10\n",
      "2799/2799 [==============================] - 13s 5ms/step - loss: 0.2122 - accuracy: 0.9161 - val_loss: 0.2074 - val_accuracy: 0.9148\n",
      "Epoch 9/10\n",
      "2799/2799 [==============================] - 12s 4ms/step - loss: 0.2059 - accuracy: 0.9170 - val_loss: 0.2046 - val_accuracy: 0.9163\n",
      "Epoch 10/10\n",
      "2799/2799 [==============================] - 12s 4ms/step - loss: 0.1974 - accuracy: 0.9203 - val_loss: 0.2050 - val_accuracy: 0.9162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19c8bf8b790>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311/311 [==============================] - 1s 3ms/step - loss: 0.3078 - accuracy: 0.8882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3077773451805115, 0.8881519436836243]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "1276247cfa5c9ee4636cf367496103f29036e30efa859b5123807facee62d98b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
