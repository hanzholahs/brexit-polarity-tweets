{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brexit Polarity Tweets - Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim import utils\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import TextVectorization, Dense, Embedding\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "EMBEDDING_DIMS = 300\n",
    "SEQ_LENGTH  = 200\n",
    "WINDOW_SIZE = 15\n",
    "EPOCHS      = 15\n",
    "MIN_COUNT   = 5\n",
    "\n",
    "# paths to data\n",
    "PATH_TRAIN = \"./data/preprocessed/train/\"\n",
    "PATH_EMBEDDINGS = \"./data/embeddings/\"\n",
    "\n",
    "tokenizer = nltk.TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper functions\n",
    "def read_tweet(filepath):\n",
    "    import pandas as pd\n",
    "    \n",
    "    tweets = []\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for tweet in f:\n",
    "            tweets.append(tweet.replace(\"\\n\", \"\"))\n",
    "    return pd.Series(tweets)\n",
    "\n",
    "def train_embedding(model_type, **kwargs):\n",
    "    if model_type.lower() == \"word2vec\":\n",
    "        model = Word2Vec(**kwargs)\n",
    "    elif model_type.lower() == \"fasttext\":\n",
    "        model = FastText(**kwargs)\n",
    "    else:\n",
    "        raise Exception(\"`model_type` must either 'word2vec' or 'fasttext'.\")\n",
    "    \n",
    "    return model.wv.index_to_key, model.wv.vectors\n",
    "\n",
    "def train_vectorizer(tweets, vocab_size, seq_length):\n",
    "    # create a variable to store frequency distribution based on label\n",
    "    fdist = nltk.FreqDist()\n",
    "\n",
    "    # calculate the frequency of tokens based on label\n",
    "    for index, tweet in zip(tweets.index, tweets):\n",
    "        for token in tokenizer.tokenize(tweet):\n",
    "            fdist[token] += 1\n",
    "    \n",
    "    vocab = [token for token, count in fdist.most_common(vocab_size)]\n",
    "    \n",
    "    return TextVectorization(output_sequence_length = seq_length,\n",
    "                             output_mode = 'int',\n",
    "                             vocabulary  = vocab)\n",
    "\n",
    "    \n",
    "def train_embedding_nn(tweets, targets, vocab_size, seq_length, embedding_dims, epochs = 5):\n",
    "    vectorizer = train_vectorizer(tweets, vocab_size, seq_length)\n",
    "    vocab = vectorizer.get_vocabulary()[2:]\n",
    "    \n",
    "    X = vectorizer(tweets)\n",
    "    y = LabelEncoder().fit_transform(targets)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Embedding(len(vocab) + 2, embedding_dims, input_shape = (seq_length,)),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(1, activation = \"sigmoid\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = \"BinaryCrossentropy\")\n",
    "    \n",
    "    model.fit(X, y, epochs = epochs, verbose = 0)\n",
    "    \n",
    "    vectors = model.layers[0].get_weights()[0][2:].astype(str)\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    return vocab, vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_embeddings(tweets, targets):\n",
    "    def tokenize_tweet(tweet):\n",
    "        return nltk.TweetTokenizer().tokenize(tweet)\n",
    "    \n",
    "    embeddings = {\"bw\": {}, \"sg\": {}, \"ft\": {}, \"nn\": {}}\n",
    "    \n",
    "    embeddings[\"bw\"][\"vocab\"], embeddings[\"bw\"][\"vectors\"] = train_embedding(\n",
    "        \"Word2Vec\",\n",
    "        sentences = tweets.apply(tokenize_tweet),\n",
    "        vector_size = EMBEDDING_DIMS,\n",
    "        window = WINDOW_SIZE,\n",
    "        min_count = MIN_COUNT,\n",
    "        epochs = EPOCHS,\n",
    "        sg = 0, # bag of words\n",
    "        workers = 10\n",
    "    )\n",
    "    \n",
    "    embeddings[\"sg\"][\"vocab\"], embeddings[\"sg\"][\"vectors\"] = train_embedding(\n",
    "        \"Word2Vec\",\n",
    "        sentences = tweets.apply(tokenize_tweet),\n",
    "        vector_size = EMBEDDING_DIMS,\n",
    "        window = WINDOW_SIZE,\n",
    "        min_count = MIN_COUNT,\n",
    "        epochs = EPOCHS,\n",
    "        sg = 1, # skipgram\n",
    "        workers = 10\n",
    "    )\n",
    "    \n",
    "    embeddings[\"ft\"][\"vocab\"], embeddings[\"ft\"][\"vectors\"] = train_embedding(\n",
    "        \"Word2Vec\",\n",
    "        sentences = tweets.apply(tokenize_tweet),\n",
    "        vector_size = EMBEDDING_DIMS,\n",
    "        window = WINDOW_SIZE,\n",
    "        min_count = MIN_COUNT,\n",
    "        epochs = EPOCHS,\n",
    "        workers = 10\n",
    "    )\n",
    "    \n",
    "    embeddings[\"nn\"][\"vocab\"], embeddings[\"nn\"][\"vectors\"] = train_embedding_nn(\n",
    "        tweets = tweets,\n",
    "        targets = targets,\n",
    "        seq_length = SEQ_LENGTH,\n",
    "        embedding_dims = EMBEDDING_DIMS,\n",
    "        vocab_size = 25000,\n",
    "        epochs = EPOCHS\n",
    "    )\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def save_embeddings(vocab, vectors, filepath):\n",
    "    assert len(vocab) == len(vectors)\n",
    "    \n",
    "    with open(filepath, \"w\") as f:\n",
    "        for word, vector in zip(vocab, vectors):\n",
    "            f.write(word + \" \")\n",
    "            f.write(\" \".join(vector) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-clean.txt\n",
      "1-clean-lemma.txt\n",
      "1-clean-negat.txt\n",
      "1-clean-nostw.txt\n",
      "2-clean-lemma-negat.txt\n",
      "2-clean-nostw-lemma.txt\n",
      "2-clean-nostw-negat.txt\n",
      "3-clean-nostw-lemma-negat.txt\n"
     ]
    }
   ],
   "source": [
    "tweet_files = [f for f in os.listdir(PATH_TRAIN) if re.match(\".*clean.*\", f)]\n",
    "targets = read_tweet(PATH_TRAIN + \"0-targets.txt\") # re-import for all rows\n",
    "\n",
    "for tweet_file in tweet_files:\n",
    "    print(tweet_file)\n",
    "\n",
    "    tweets = read_tweet(PATH_TRAIN + tweet_file)\n",
    "    embeddings = get_all_embeddings(tweets, targets)    \n",
    "    \n",
    "    for key in embeddings.keys():\n",
    "        save_embeddings(embeddings[key][\"vocab\"], \n",
    "                        embeddings[key][\"vectors\"].astype(str),\n",
    "                        PATH_EMBEDDINGS + key + \"-\" + tweet_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
