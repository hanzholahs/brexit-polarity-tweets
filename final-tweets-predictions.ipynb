{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The Brexit was a term that refers to the withdrawal of the United Kingdom (UK) from the European Union (EU) after 40 years of relationship. Officially, the UK left on 31 January 2020, marking it as the first and sole country to ever left the EU. The term 'Brexit' refers to a combination of words *Britain* and *exit*. As Brexit has significant implications to the people of the UK, diversing opinions (positively and negatively) arose with the event. Some argue the merits of Brexit including more control over democracy, borders, and money that would improve several areas, e.g., healthcare, costumer rights, and environment. On the other end, people opposes the idea as the decision impact negatively in trade, migration, and investments. This complexity and delicacy are present in the  social media discussions such as in Twitter.\n",
    "\n",
    "This is the first part of on the analysis of Brexit polarity tweets, which is the exploratory analysis part. The project aims to build a neural network-based classifier to predict whether a tweet is created by a user who supports or opposes Brexit. This analysis leverage data from Kaggle: [Brexit Polarity Tweets](https://www.kaggle.com/datasets/visalakshiiyer/twitter-data-brexit). \n",
    "\n",
    "The project's Github repository can be accessed [here](https://www.github.com/hanzholahs/brexit-polarity-tweets).\n",
    "\n",
    "\n",
    "#### About the dataset\n",
    "\n",
    "These datasets were collated as part of a dissertation project. This Twitter dataset covers the January - March 2022 period and comprises tweets relating to Brexit or Europe from Twitter accounts with publicly stated Brexit positions in their bio. It was collected using Boolean search for both types of users.\n",
    "\n",
    "The Boolean search for **pro-Brexit** tweet is:\n",
    "\n",
    "[(bio:\"Brexit support\" OR bio:\"pro-brexit\" OR bio:\"pro brexit\" OR bio:\"Pro #Brexit\" OR bio:brexiteer OR bio:probrexit) AND (EU OR Brexit OR CUSTOMS OR EUROPEAN OR EUROPE OR #Remain OR *Brexit OR #rejoinEU)]{style=\"font-family:Consolas,Monaco,Lucida Console,Liberation Mono,DejaVu Sans Mono,Bitstream Vera Sans Mono,Courier New;font-size:75%\"}\n",
    "\n",
    "\n",
    "The Boolean search for **anti-Brexit** tweet is: \n",
    "\n",
    "[(bio:\"anti brexit\" OR bio:\"anti-brexit\" OR bio:\"antibrexit\" OR bio:\"Pro remain\" OR bio:\"pro-remain\" OR bio:remainer) AND (EU OR BREXIT OR CUSTOMS OR EUROPEAN OR EUROPE OR #Remain OR *Brexit)]{style=\"font-family:Consolas,Monaco,Lucida Console,Liberation Mono,DejaVu Sans Mono,Bitstream Vera Sans Mono,Courier New;font-size:75%\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8kzg4h68OI0"
   },
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "The notebook was run on the Google Colab platform which provides additional functionalities such as Google Drive connectivity and pre-installed Kaggle API. For setting up the analysis, several task was performed, including:\n",
    "\n",
    "* Mounting to google drive for Kaggle API credential\n",
    "* Downloading data directly from [Kaggle](https://www.kaggle.com/) using Kaggle API\n",
    "* Downloading GloVe6B dataset for embedding language data\n",
    "* Importing essential libraries (Numpy, Pandas, Scikit-learn, Tensorflow2, etc.)\n",
    "* Specifying some constant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_gnW5tfV7t6C"
   },
   "outputs": [],
   "source": [
    "# mount gdrive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpskFzPK5buM"
   },
   "outputs": [],
   "source": [
    "# download Brexit dataset\n",
    "!mkdir ~/.kaggle\n",
    "!cp /content/drive/MyDrive/.credentials/kaggle.json ~/.kaggle\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download visalakshiiyer/twitter-data-brexit\n",
    "!unzip -d data/ twitter-data-brexit.zip \n",
    "\n",
    "# download GloVe6B dataset\n",
    "!wget 'https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.zip'\n",
    "\n",
    "from shutil import unpack_archive\n",
    "import os\n",
    "\n",
    "# unzip file\n",
    "unpack_archive('glove.6B.zip')\n",
    "os.remove('glove.6B.300d.txt')\n",
    "os.remove('glove.6B.200d.txt')\n",
    "# os.remove('glove.6B.100d.txt')\n",
    "os.remove('glove.6B.50d.txt')\n",
    "os.remove('glove.6B.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_loXZ40-3bU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, GRU, SimpleRNN, \n",
    "    Dense, Dropout\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jz9v3-NDEfRQ"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "clear_output()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords  = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfCKKAjq_G-P",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# variables related to dataset and glove data\n",
    "INPUT_PATH_ANTI  = '/content/data/TweetDataset_AntiBrexit_Jan-Mar2022.csv'\n",
    "INPUT_PATH_PRO   = '/content/data/TweetDataset_ProBrexit_Jan-Mar2022.csv'\n",
    "INPUT_PATH_GLOVE = '/content/glove.6B.100d.txt'\n",
    "\n",
    "# variables related to model checkpoints\n",
    "HISTORY_PATH    = '/content/drive/MyDrive/Projects/Big Projects/Sentiment Analysis using Deep Learning/history/history.pkl'\n",
    "CHECKPOINT_PATH = '/content/drive/MyDrive/Projects/Big Projects/Sentiment Analysis using Deep Learning/checkpoint/cp-{epoch:02d}.ckpt'\n",
    "CHECKPOINT_DIR  = os.path.dirname(CHECKPOINT_PATH)\n",
    "\n",
    "# variables related to modelling process\n",
    "num_words = 30_000\n",
    "row_limit = 100_000\n",
    "embedding_dim = 100\n",
    "test_split = 0.10\n",
    "val_split  = 0.10 / 0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nScPCQAM8nR-"
   },
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "As all setup completed, we can prepare the data for training the model. This is done in several steps ranging from importing the dataset to cleaning and tokenizing data to embedding words. Specifically, the process of preparing data includes:\n",
    "\n",
    "* Importing data (pro and anti tweets)\n",
    "* Sampling with a specified size (in this case the sample size is 100,000)\n",
    "* Cleaning data (remove unwanted parts such as emoticon and URLs, lemmatization, etc.)\n",
    "* Splitting data into `train`, `test`, and `validation`\n",
    "* Embedding words using the pre-trained [GloVe Embeddings](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDCTDUr09M-c"
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "tweets_pro  = pd.read_csv(INPUT_PATH_PRO)\n",
    "tweets_anti = pd.read_csv(INPUT_PATH_ANTI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Op4bGYKzS0u"
   },
   "outputs": [],
   "source": [
    "# specify sample indices from data\n",
    "ind_pro  = np.random.choice(len(tweets_pro), replace = False, size = row_limit)\n",
    "ind_anti = np.random.choice(len(tweets_anti), replace = False, size = row_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGw5bkUyA-8b"
   },
   "outputs": [],
   "source": [
    "# binds pro-brexit and anti-brexit tables\n",
    "assert np.mean(tweets_pro.dtypes == tweets_anti.dtypes) == 1\n",
    "assert np.mean(tweets_pro.columns == tweets_anti.columns) == 1\n",
    "\n",
    "# create dataset for modelling\n",
    "tweets = pd.concat([tweets_pro[\"Hit Sentence\"][ind_pro], \n",
    "                    tweets_anti[\"Hit Sentence\"][ind_anti]])\n",
    "tweets = tweets.reset_index(drop = True)\n",
    "labels = pd.Series(np.concatenate([np.repeat([\"Pro\"], row_limit),\n",
    "                                   np.repeat([\"Anti\"], row_limit)]))\n",
    "\n",
    "del tweets_pro\n",
    "del tweets_anti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQQI4NLtEOSO"
   },
   "outputs": [],
   "source": [
    "# Create pre-processing functions\n",
    "def remove_qt_rt_uname(text):\n",
    "    qt_rt = re.compile(r'(RT|QT)? ?@[\\w]+:?')\n",
    "    return qt_rt.sub(r'', text)\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    return html.sub(r'', text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_stopwords(text, stopwords = stopwords):\n",
    "    return \" \".join([w for w in text.split(\" \") if w.lower() not in stopwords])\n",
    "\n",
    "def lemmatize(text, lemmatizer = lemmatizer):\n",
    "    return \" \".join([lemmatizer.lemmatize(w) for w in text.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W04EC6ZEjLSs"
   },
   "outputs": [],
   "source": [
    "# Pre-process data\n",
    "tweets = tweets.apply(lambda tweet: remove_qt_rt_uname(tweet)) \\\n",
    "    .apply(lambda tweet: remove_URL(tweet)) \\\n",
    "    .apply(lambda tweet: remove_emoji(tweet)) \\\n",
    "    .apply(lambda tweet: remove_html(tweet)) \\\n",
    "    .apply(lambda tweet: remove_stopwords(tweet)) \\\n",
    "    .apply(lambda tweet: remove_punct(tweet)) \\\n",
    "    .apply(lambda tweet: lemmatize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0WNU-7Piu5xJ"
   },
   "outputs": [],
   "source": [
    "if len(tweets) >= 50000:\n",
    "    test_split = 5000 / len(tweets)\n",
    "    val_split  = 5000 / (len(tweets) * (1 - test_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpaWJtmUFe_w",
    "outputId": "ce32be12-8cf8-4248-b1b3-84aba987641a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Size = 190000\n",
      "Test Dataset Size  = 5000\n",
      "Val Dataset Size   = 5000\n"
     ]
    }
   ],
   "source": [
    "# Split data into train, test, and validation\n",
    "X_train, X_test, y_train, y_test  = train_test_split(\n",
    "    tweets, \n",
    "    labels, \n",
    "    stratify = labels, \n",
    "    test_size = test_split, \n",
    "    random_state = 321\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    stratify = y_train, \n",
    "    test_size = val_split, \n",
    "    random_state = 321)\n",
    "\n",
    "\n",
    "# Check Data\n",
    "print(f\"Train Dataset Size = {len(X_train)}\")\n",
    "print(f\"Test Dataset Size  = {len(X_test)}\")\n",
    "print(f\"Val Dataset Size   = {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XPhemmuiGHZ7",
    "outputId": "5104e026-717f-43ff-f0b5-0d54a5e1ac1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size = 76835\n",
      "X train shape   = (190000, 256)\n",
      "X val shape     = (5000,)\n",
      "X test shape    = (190000,)\n",
      "y train shape   = (5000, 256)\n",
      "y val shape     = (5000,)\n",
      "y test shape    = (5000,)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize words\n",
    "tokenizer = Tokenizer(num_words = num_words, oov_token = \"<<OOV>>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "\n",
    "# Create sequences\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test  = tokenizer.texts_to_sequences(X_test)\n",
    "sequences_val   = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "X_train = pad_sequences(sequences_train, maxlen=256, truncating='pre')\n",
    "X_test  = pad_sequences(sequences_test, maxlen=256, truncating='pre')\n",
    "X_val   = pad_sequences(sequences_val, maxlen=256, truncating='pre')\n",
    "\n",
    "\n",
    "# Encode labels\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test  = label_encoder.fit_transform(y_test)\n",
    "y_val   = label_encoder.fit_transform(y_val)\n",
    "\n",
    "\n",
    "# Check data\n",
    "vocabSize = len(tokenizer.index_word) + 1\n",
    "print(f\"Vocabulary size = {vocabSize}\")\n",
    "print(f\"X train shape   = {X_train.shape}\")\n",
    "print(f\"X val shape     = {y_val.shape}\")\n",
    "print(f\"X test shape    = {y_train.shape}\")\n",
    "print(f\"y train shape   = {X_test.shape}\")\n",
    "print(f\"y val shape     = {y_val.shape}\")\n",
    "print(f\"y test shape    = {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H88zMET_gXnz",
    "outputId": "5a6cddbe-0311-412d-8976-9dbcb4525456"
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "num_tokens = vocabSize\n",
    "\n",
    "# Read word vectors\n",
    "with open(INPUT_PATH_GLOVE) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "\n",
    "# Assign word vectors to our dictionary/vocabulary\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcTY-SCN9vDs"
   },
   "source": [
    "## 3. Model Building\n",
    "\n",
    "The next step is to build the model. This process contains several tasks. \n",
    "\n",
    "* The first thing is to configure relevant aspects with regards to training. Here I define five callback functions: early stopping, learning rate scheduler, learning rate reducer, model checkpoint, and training terminator given `NaN` loss value.\n",
    "* The second task is to define the model. I wrote `create_model` function as the function to generate the model. I defined the model architecture combining convolutional and recurrent layer types with some attributes.\n",
    "    * An embedding layer to convert input sequences into its vector representation based on GloVe embedding whose weights are updated during training\n",
    "    * Two layers of 1-D convolutional layer followed by pooling layer based on maximum value\n",
    "    * A RNN layer\n",
    "    * A dense layer\n",
    "    * A L-2 regularizers which is implemented in the kernel and recurrent regularizers\n",
    "    * Some dropouts layers\n",
    "* Lastly, model is trained with a maximum of 30 epochs using training data and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1uC8bsMZ6Wv"
   },
   "outputs": [],
   "source": [
    "# define callback functions\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    min_delta = 0.001,\n",
    "    patience = 10\n",
    ")\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch, lr: lr if epoch < 25 else lr * tf.math.exp(-0.01)\n",
    ")\n",
    "\n",
    "lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor = 'val_loss',\n",
    "    factor = 0.9,\n",
    "    patience = 3,\n",
    "    min_delta = 0.001,\n",
    ")\n",
    "\n",
    "check_point = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = CHECKPOINT_PATH,\n",
    "    verbose = 0, \n",
    "    save_weights_only = True,\n",
    "    save_freq='epoch'\n",
    ")\n",
    "\n",
    "nan_terminator = tf.keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "callbacks     = [\n",
    "    early_stopping, \n",
    "    lr_scheduler, \n",
    "    lr_reducer, \n",
    "    nan_terminator, \n",
    "    check_point\n",
    "]\n",
    "\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OmZ13rBtWCWt",
    "outputId": "631ffcb0-a743-4b4c-a686-283ada7ad422"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, None, 100)         7683500   \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, None, 64)          19264     \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, None, 64)          0         \n",
      "                                                                 \n",
      " max_pooling1d_6 (MaxPooling  (None, None, 64)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, None, 216)         41688     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, None, 216)         0         \n",
      "                                                                 \n",
      " max_pooling1d_7 (MaxPooling  (None, None, 216)        0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " simple_rnn_3 (SimpleRNN)    (None, 128)               44160     \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1024)              132096    \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,921,733\n",
      "Trainable params: 7,921,733\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(num_tokens, embedding_dim, embedding_matrix):\n",
    "    regularizer = tf.keras.regularizers.l2(0.0001)\n",
    "    embeddings  = tf.keras.initializers.Constant(embedding_matrix)\n",
    "    loss        = tf.keras.losses.BinaryCrossentropy()\n",
    "    optimizer   = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "    metrics     = [\"accuracy\"]\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        Embedding(num_tokens,\n",
    "                  embedding_dim,\n",
    "                  embeddings_initializer = embeddings,\n",
    "                  trainable = True),\n",
    "        Conv1D(filters = 64,\n",
    "               kernel_size = 3,\n",
    "               padding = \"causal\",\n",
    "               activation = \"relu\"),\n",
    "        Dropout(0.4),\n",
    "        MaxPooling1D(pool_size = 2),\n",
    "        Conv1D(filters = 216,\n",
    "               kernel_size = 3,\n",
    "               padding = \"causal\",\n",
    "               activation = \"relu\"),\n",
    "        Dropout(0.4),\n",
    "        MaxPooling1D(pool_size = 2),\n",
    "        SimpleRNN(128, \n",
    "                  activation = \"relu\", \n",
    "                  kernel_regularizer = regularizer,\n",
    "                  recurrent_regularizer = regularizer),\n",
    "        Dropout(0.4),\n",
    "        Dense(1024, activation='relu', kernel_regularizer = regularizer),\n",
    "        Dropout(0.4),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss = loss,\n",
    "                  optimizer = optimizer,\n",
    "                  metrics = metrics)\n",
    "\n",
    "    return model\n",
    "  \n",
    "model = create_model(num_tokens, embedding_dim, embedding_matrix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H8Owac30ZtA-",
    "outputId": "e86b2e94-1359-4790-c61e-8ecd0e460eed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "5938/5938 [==============================] - 407s 68ms/step - loss: 0.6638 - accuracy: 0.6349 - val_loss: 0.5620 - val_accuracy: 0.7490 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "5938/5938 [==============================] - 406s 68ms/step - loss: 0.4963 - accuracy: 0.7835 - val_loss: 0.4800 - val_accuracy: 0.8124 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "5938/5938 [==============================] - 402s 68ms/step - loss: 0.4224 - accuracy: 0.8249 - val_loss: 0.4390 - val_accuracy: 0.8328 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "5938/5938 [==============================] - 398s 67ms/step - loss: 0.3820 - accuracy: 0.8451 - val_loss: 0.4043 - val_accuracy: 0.8382 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "5938/5938 [==============================] - 396s 67ms/step - loss: 0.3552 - accuracy: 0.8584 - val_loss: 0.3784 - val_accuracy: 0.8490 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "5938/5938 [==============================] - 399s 67ms/step - loss: 0.3334 - accuracy: 0.8683 - val_loss: 0.3654 - val_accuracy: 0.8558 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "5938/5938 [==============================] - 399s 67ms/step - loss: 0.3187 - accuracy: 0.8740 - val_loss: 0.3562 - val_accuracy: 0.8584 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "5938/5938 [==============================] - 401s 68ms/step - loss: 0.3070 - accuracy: 0.8798 - val_loss: 0.3542 - val_accuracy: 0.8598 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "5938/5938 [==============================] - 403s 68ms/step - loss: 0.2959 - accuracy: 0.8854 - val_loss: 0.3419 - val_accuracy: 0.8642 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "5938/5938 [==============================] - 402s 68ms/step - loss: 0.2872 - accuracy: 0.8883 - val_loss: 0.3418 - val_accuracy: 0.8652 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "5938/5938 [==============================] - 398s 67ms/step - loss: 0.2781 - accuracy: 0.8931 - val_loss: 0.3390 - val_accuracy: 0.8682 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "5938/5938 [==============================] - 400s 67ms/step - loss: 0.2709 - accuracy: 0.8953 - val_loss: 0.3362 - val_accuracy: 0.8690 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "5938/5938 [==============================] - 396s 67ms/step - loss: 0.2654 - accuracy: 0.8985 - val_loss: 0.3373 - val_accuracy: 0.8704 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "5938/5938 [==============================] - 397s 67ms/step - loss: 0.2594 - accuracy: 0.9012 - val_loss: 0.3358 - val_accuracy: 0.8696 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "5938/5938 [==============================] - 396s 67ms/step - loss: 0.2532 - accuracy: 0.9042 - val_loss: 0.3396 - val_accuracy: 0.8668 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "5938/5938 [==============================] - 395s 67ms/step - loss: 0.2475 - accuracy: 0.9065 - val_loss: 0.3390 - val_accuracy: 0.8672 - lr: 9.0000e-05\n",
      "Epoch 17/30\n",
      "5938/5938 [==============================] - 395s 66ms/step - loss: 0.2432 - accuracy: 0.9082 - val_loss: 0.3370 - val_accuracy: 0.8698 - lr: 9.0000e-05\n",
      "Epoch 18/30\n",
      "5938/5938 [==============================] - 400s 67ms/step - loss: 0.2392 - accuracy: 0.9096 - val_loss: 0.3379 - val_accuracy: 0.8682 - lr: 9.0000e-05\n",
      "Epoch 19/30\n",
      "5938/5938 [==============================] - 404s 68ms/step - loss: 0.2333 - accuracy: 0.9122 - val_loss: 0.3355 - val_accuracy: 0.8692 - lr: 8.1000e-05\n",
      "Epoch 20/30\n",
      "5938/5938 [==============================] - 398s 67ms/step - loss: 0.2299 - accuracy: 0.9144 - val_loss: 0.3375 - val_accuracy: 0.8684 - lr: 8.1000e-05\n",
      "Epoch 21/30\n",
      "5938/5938 [==============================] - 396s 67ms/step - loss: 0.2268 - accuracy: 0.9153 - val_loss: 0.3382 - val_accuracy: 0.8696 - lr: 8.1000e-05\n",
      "Epoch 22/30\n",
      "5938/5938 [==============================] - 393s 66ms/step - loss: 0.2227 - accuracy: 0.9172 - val_loss: 0.3427 - val_accuracy: 0.8668 - lr: 7.2900e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs = 30, \n",
    "    validation_data = (X_val, y_val), \n",
    "    verbose = 1,\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation\n",
    "\n",
    "Trained model is evaluated using testing data. The results show that the model is capable to predict the classification of tweets related to Brexit with the score of 86.3% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rY4lrR-3Z2UR",
    "outputId": "7f9ac855-1473-44a3-f2cd-f86155fafb4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 8ms/step - loss: 0.3450 - accuracy: 0.8632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34497830271720886, 0.8632000088691711]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7B_HIiw-Rpu"
   },
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "Based on the whole process, it can be concluded that a Deep Learning model could find the pattern to differentiate between pro- and anti-Brexit tweets. The model can predict with about 86 percent accuracy. Furthermore, combining between convolutional and recurrent network is proven to be working for this type of data. The different architectures were also attempted to produce (e.g., pure neural network, pure recurrent neural network, pure convolutional neural network, and neural networks with LSTM layers), but most were not as optimal as this architecture in terms of model performance and training speed. The analysis also showed that pre-trained word embeddings can be used in training a deep learning model with natural language data. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "1276247cfa5c9ee4636cf367496103f29036e30efa859b5123807facee62d98b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
