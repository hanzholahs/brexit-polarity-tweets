{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "# Data Manipulation and Visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP Tools\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.sentiment.util import mark_negation\n",
    "\n",
    "# ML Tools\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify some NLP-related functions \n",
    "label_encoder = LabelEncoder()\n",
    "tokenizer     = TweetTokenizer()\n",
    "lemmatizer    = WordNetLemmatizer()\n",
    "\n",
    "# specify unwanted tokens to be filtered in cleaning process\n",
    "def get_stopwords():\n",
    "    stopwords = [w.lower() for w in nltk.corpus.stopwords.words('english')]\n",
    "    lemmatized_stopwords = [lemmatizer.lemmatize(word) for word in stopwords]\n",
    "    return stopwords + lemmatized_stopwords\n",
    "\n",
    "def get_punctuations():\n",
    "    no_punctuations_stopwords = [re.sub(f\"[{string.punctuation}]\", \"\", word) for word in stopwords]\n",
    "    custom = [\"…\", \"...\", \"..\", \"\\xad\", \"–\"]\n",
    "    punctuations = list(string.punctuation)\n",
    "    return no_punctuations_stopwords + custom + punctuations\n",
    "\n",
    "stopwords = get_stopwords()\n",
    "punctuations = get_punctuations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "N_ROWS     = None # `None` to import all rows\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "# paths to data\n",
    "PATH_DATA  = \"./data/\"\n",
    "PATH_TRAIN = PATH_DATA + \"preprocessed/train/\"\n",
    "PATH_TEST  = PATH_DATA + \"preprocessed/test/\"\n",
    "PATH_ANTI  = PATH_DATA + 'raw/TweetDataset_AntiBrexit_Jan-Mar2022.csv'\n",
    "PATH_PRO   = PATH_DATA + 'raw/TweetDataset_ProBrexit_Jan-Mar2022.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 94652 tweets from users who support Brexit\n",
      "There are 102442 tweets from users who doesn't support Brexit\n"
     ]
    }
   ],
   "source": [
    "# import both pro and anti tweets data\n",
    "if N_ROWS is not None:\n",
    "    nrows = N_ROWS // 2\n",
    "else:\n",
    "    nrows = None\n",
    "\n",
    "pro  = pd.read_csv(PATH_PRO, nrows = nrows)[\"Hit Sentence\"].drop_duplicates()\n",
    "anti = pd.read_csv(PATH_ANTI, nrows = nrows)[\"Hit Sentence\"].drop_duplicates()\n",
    "\n",
    "# combine all tweets data and create targets variable\n",
    "tweets = pd.concat([pro, anti]).reset_index(drop = True)\n",
    "targets = pd.Series(np.repeat([\"Pro\", \"Anti\"], [len(pro), len(anti)]))\n",
    "\n",
    "# count the total number of tweets\n",
    "tweets_length = len(pro) + len(anti)\n",
    "\n",
    "print(f\"There are {len(pro)} tweets from users who support Brexit\")\n",
    "print(f\"There are {len(anti)} tweets from users who doesn't support Brexit\")\n",
    "\n",
    "# delete unused variables\n",
    "del pro, anti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    \"\"\"Clean all tweets from RT, QT, URLs, HTML Tags, numbers and statistics, emojis, and unicode punctuation marks\"\"\"\n",
    "    \n",
    "    # Regex for removing \"RT\" or \"QT\"\n",
    "    qt_rt = re.compile(r'(RT|QT)? ?@[\\w]+:?')\n",
    "\n",
    "    # Regex for removing URL\n",
    "    url = re.compile(r'https?://\\S+')\n",
    "\n",
    "    # Regex for removing HTML tags\n",
    "    # source of regex: https://stackoverflow.com/a/12982689\n",
    "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "\n",
    "    # Regex for removing numbers and statistics\n",
    "    stats = re.compile(\"[$|£]?.?[0-9]+(?:,?[0-9]{3})*(?:\\.[0-9]*)*%?(?:st|nd|rd|th)?\")\n",
    "\n",
    "    # Regex for removing emojis\n",
    "    # Source of regex: https://stackoverflow.com/a/58356570\n",
    "    emojis = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "        \"]+\", flags = re.UNICODE)\n",
    "\n",
    "    # Regex for replacing unicode quotation marks into regular one\n",
    "    single_quotation = re.compile(u\"[\\u2018|\\u2019]\", flags = re.UNICODE)\n",
    "    double_quotation = re.compile(u\"[\\u201c|\\u201d]\", flags = re.UNICODE)\n",
    "    \n",
    "    tweet = re.sub(qt_rt, \"\", tweet)\n",
    "    tweet = re.sub(url, \"\", tweet)\n",
    "    tweet = re.sub(html, \"\", tweet)\n",
    "    tweet = re.sub(stats, \"\", tweet)\n",
    "    tweet = re.sub(emojis, \"\", tweet)\n",
    "    tweet = re.sub(single_quotation, \"'\", tweet)\n",
    "    tweet = re.sub(double_quotation, \"\\\"\", tweet)\n",
    "    tweet = re.sub(\" +\", \" \", tweet).strip()\n",
    "    \n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    tokens = [t.lower() for t in tokens if t.lower() not in punctuations]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def remove_stopwords(tweet):\n",
    "    return \" \".join([t.lower() for t in tokenizer.tokenize(tweet) if t.lower() not in stopwords])\n",
    "\n",
    "def lemmatize(tweet):\n",
    "    return \" \".join([lemmatizer.lemmatize(t) for t in tokenizer.tokenize(tweet)])\n",
    "\n",
    "def mark_negation(tweet):\n",
    "    mark = nltk.sentiment.util.mark_negation\n",
    "    return \" \".join(mark([t for t in tokenizer.tokenize(tweet)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.apply(clean_tweet)\n",
    "tweets_nostw = tweets.apply(remove_stopwords)\n",
    "tweets_lemma = tweets.apply(lemmatize)\n",
    "tweets_negat = tweets.apply(mark_negation)\n",
    "tweets_nostw_lemma = tweets_nostw.apply(lemmatize)\n",
    "tweets_lemma_negat = tweets_lemma.apply(mark_negation)\n",
    "tweets_nostw_negat = tweets_nostw.apply(mark_negation)\n",
    "tweets_nostw_lemma_negat = tweets_nostw_lemma.apply(mark_negation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 1087 out of 197094 tweets. There are 196007 tweets left.\n"
     ]
    }
   ],
   "source": [
    "non_empty_tweets = tweets_nostw_lemma_negat.map(len) > 0\n",
    "\n",
    "targets = targets[non_empty_tweets].reset_index(drop = True)\n",
    "tweets = tweets[non_empty_tweets].reset_index(drop = True)\n",
    "tweets_nostw = tweets_nostw[non_empty_tweets].reset_index(drop = True)\n",
    "tweets_lemma = tweets_lemma[non_empty_tweets].reset_index(drop = True)\n",
    "tweets_negat = tweets_negat[non_empty_tweets].reset_index(drop = True)\n",
    "tweets_nostw_lemma = tweets_nostw_lemma[non_empty_tweets].reset_index(drop = True)\n",
    "tweets_lemma_negat = tweets_lemma_negat[non_empty_tweets].reset_index(drop = True)\n",
    "tweets_nostw_negat = tweets_nostw_negat[non_empty_tweets].reset_index(drop = True)\n",
    "tweets_nostw_lemma_negat = tweets_nostw_lemma_negat[non_empty_tweets].reset_index(drop = True)\n",
    "\n",
    "# count the total number of tweets left\n",
    "print(f\"Deleted {tweets_length - len(tweets)} out of {tweets_length} tweets.\", end = \" \")\n",
    "print(f\"There are {len(tweets)} tweets left.\")\n",
    "tweets_length = len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 156805 tweets in the train dataset.\n",
      "There are 39202 tweets in the test dataset.\n"
     ]
    }
   ],
   "source": [
    "# generate indices to split dataset\n",
    "sss = ShuffleSplit(n_splits = 1, test_size = TEST_SPLIT, random_state = 123)\n",
    "sss.get_n_splits(tweets, targets)\n",
    "\n",
    "train_index, test_index = next(sss.split(tweets, targets))\n",
    "\n",
    "print(f\"There are {len(train_index)} tweets in the train dataset.\")\n",
    "print(f\"There are {len(test_index)} tweets in the test dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tweets(tweets, filepath):\n",
    "    with open(filepath, \"w\") as f:\n",
    "        for tweet in tweets:\n",
    "            f.write(tweet + \"\\n\")\n",
    "\n",
    "save_tweets(targets[train_index], PATH_TRAIN + \"0-targets.txt\")\n",
    "save_tweets(tweets[train_index], PATH_TRAIN + \"0-clean.txt\")\n",
    "save_tweets(tweets_nostw[train_index], PATH_TRAIN + \"1-clean-nostw.txt\")\n",
    "save_tweets(tweets_lemma[train_index], PATH_TRAIN + \"1-clean-lemma.txt\")\n",
    "save_tweets(tweets_negat[train_index], PATH_TRAIN + \"1-clean-negat.txt\")\n",
    "save_tweets(tweets_nostw_lemma[train_index], PATH_TRAIN + \"2-clean-nostw-lemma.txt\")\n",
    "save_tweets(tweets_lemma_negat[train_index], PATH_TRAIN + \"2-clean-lemma-negat.txt\")\n",
    "save_tweets(tweets_nostw_negat[train_index], PATH_TRAIN + \"2-clean-nostw-negat.txt\")\n",
    "save_tweets(tweets_nostw_lemma_negat[train_index], PATH_TRAIN + \"3-clean-nostw-lemma-negat.txt\")\n",
    "\n",
    "save_tweets(targets[test_index], PATH_TEST + \"0-targets.txt\")\n",
    "save_tweets(tweets[test_index], PATH_TEST + \"0-clean.txt\")\n",
    "save_tweets(tweets_nostw[test_index], PATH_TEST + \"1-clean-nostw.txt\")\n",
    "save_tweets(tweets_lemma[test_index], PATH_TEST + \"1-clean-lemma.txt\")\n",
    "save_tweets(tweets_negat[test_index], PATH_TEST + \"1-clean-negat.txt\")\n",
    "save_tweets(tweets_nostw_lemma[test_index], PATH_TEST + \"2-clean-nostw-lemma.txt\")\n",
    "save_tweets(tweets_lemma_negat[test_index], PATH_TEST + \"2-clean-lemma-negat.txt\")\n",
    "save_tweets(tweets_nostw_negat[test_index], PATH_TEST + \"2-clean-nostw-negat.txt\")\n",
    "save_tweets(tweets_nostw_lemma_negat[test_index], PATH_TEST + \"3-clean-nostw-lemma-negat.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
